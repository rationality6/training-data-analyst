{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_test.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rationality6/training-data-analyst/blob/master/GAN_test.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Ql85DAgtjAgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2081cdaf-2062-4b26-9f9b-bded932b1569"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    99% |████████████████████████████████| 484.0MB 36.2MB/s eta 0:00:01"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-EJtue3ijElB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "bf490b62-6e1a-47bc-d67f-1d9681d81139"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Data params\n",
        "data_mean = 4\n",
        "data_stddev = 1.25\n",
        "\n",
        "# Model params\n",
        "g_input_size = 1     # Random noise dimension coming into generator, per output vector\n",
        "g_hidden_size = 50   # Generator complexity\n",
        "g_output_size = 1    # size of generated output vector\n",
        "d_input_size = 100   # Minibatch size - cardinality of distributions\n",
        "d_hidden_size = 50   # Discriminator complexity\n",
        "d_output_size = 1    # Single dimension for 'real' vs. 'fake'\n",
        "minibatch_size = d_input_size\n",
        "\n",
        "d_learning_rate = 2e-4  # 2e-4\n",
        "g_learning_rate = 2e-4\n",
        "optim_betas = (0.9, 0.999)\n",
        "num_epochs = 30000\n",
        "print_interval = 200\n",
        "d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
        "g_steps = 1\n",
        "\n",
        "# ### Uncomment only one of these\n",
        "#(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
        "(name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
        "\n",
        "print(\"Using data [%s]\" % (name))\n",
        "\n",
        "# ##### DATA: Target data and generator input data\n",
        "\n",
        "def get_distribution_sampler(mu, sigma):\n",
        "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))  # Gaussian\n",
        "\n",
        "def get_generator_input_sampler():\n",
        "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
        "\n",
        "# ##### MODELS: Generator model and discriminator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.elu(self.map1(x))\n",
        "        x = F.sigmoid(self.map2(x))\n",
        "        return self.map3(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.elu(self.map1(x))\n",
        "        x = F.elu(self.map2(x))\n",
        "        return F.sigmoid(self.map3(x))\n",
        "\n",
        "def extract(v):\n",
        "    return v.data.storage().tolist()\n",
        "\n",
        "def stats(d):\n",
        "    return [np.mean(d), np.std(d)]\n",
        "\n",
        "def decorate_with_diffs(data, exponent):\n",
        "    mean = torch.mean(data.data, 1, keepdim=True)\n",
        "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
        "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
        "    return torch.cat([data, diffs], 1)\n",
        "\n",
        "d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
        "gi_sampler = get_generator_input_sampler()\n",
        "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
        "D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)\n",
        "criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
        "d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for d_index in range(d_steps):\n",
        "        # 1. Train D on real+fake\n",
        "        D.zero_grad()\n",
        "\n",
        "        #  1A: Train D on real\n",
        "        d_real_data = Variable(d_sampler(d_input_size))\n",
        "        d_real_decision = D(preprocess(d_real_data))\n",
        "        d_real_error = criterion(d_real_decision, Variable(torch.ones(1)))  # ones = true\n",
        "        d_real_error.backward() # compute/store gradients, but don't change params\n",
        "\n",
        "        #  1B: Train D on fake\n",
        "        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "        d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
        "        d_fake_decision = D(preprocess(d_fake_data.t()))\n",
        "        d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1)))  # zeros = fake\n",
        "        d_fake_error.backward()\n",
        "        d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
        "\n",
        "    for g_index in range(g_steps):\n",
        "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
        "        G.zero_grad()\n",
        "\n",
        "        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
        "        g_fake_data = G(gen_input)\n",
        "        dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
        "        g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))  # we want to fool, so pretend it's all genuine\n",
        "\n",
        "        g_error.backward()\n",
        "        g_optimizer.step()  # Only optimizes G's parameters\n",
        "\n",
        "    if epoch % print_interval == 0:\n",
        "        print(\"%s: D: %s/%s G: %s (Real: %s, Fake: %s) \" % (epoch,\n",
        "                                                            extract(d_real_error)[0],\n",
        "                                                            extract(d_fake_error)[0],\n",
        "                                                            extract(g_error)[0],\n",
        "                                                            stats(extract(d_real_data)),\n",
        "                                                            stats(extract(d_fake_data))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using data [Data and variances]\n",
            "0: D: 0.36695966124534607/0.7361935973167419 G: 0.6472253799438477 (Real: [4.097735848426819, 1.4345880970118199], Fake: [0.34810964167118075, 0.013683254241574898]) \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200: D: 1.1920829479095119e-07/0.25524982810020447 G: 1.5029319524765015 (Real: [3.9704337564110754, 1.4202614207090427], Fake: [-0.5405462372303009, 0.049636636296139895]) \n",
            "400: D: 1.6689438780304044e-05/0.41314446926116943 G: 1.127220630645752 (Real: [4.059765149354934, 1.1050084499505262], Fake: [-0.40176621958613395, 0.1270062558462856]) \n",
            "600: D: 1.0252050742565189e-05/0.25516918301582336 G: 1.3740943670272827 (Real: [3.968205041885376, 1.3021753077322031], Fake: [-0.40745228946208956, 0.27306998702320906]) \n",
            "800: D: 0.03467608243227005/0.0277311522513628 G: 3.6308650970458984 (Real: [3.939725151062012, 1.1743076998736635], Fake: [0.6639156720042229, 0.32889664519198586]) \n",
            "1000: D: 0.0010140909580513835/0.016681784763932228 G: 4.260204315185547 (Real: [3.8619229471683503, 1.2233859935618865], Fake: [0.12705163786187768, 0.6310469417199044]) \n",
            "1200: D: 0.05293547734618187/0.1826736032962799 G: 1.9480687379837036 (Real: [3.9891818815469744, 1.285520032470656], Fake: [2.7354997116327286, 0.931213822419895]) \n",
            "1400: D: 0.39927035570144653/1.617323398590088 G: 0.4407114088535309 (Real: [3.864618252515793, 1.3114140638936038], Fake: [3.7624598586559297, 1.5444361356537326]) \n",
            "1600: D: 1.8477239608764648/2.5494818687438965 G: 2.099452495574951 (Real: [4.009650049433112, 1.3084828251888398], Fake: [4.872642325162888, 1.5683376370684212]) \n",
            "1800: D: 0.8927111029624939/0.9322298169136047 G: 1.0355522632598877 (Real: [4.056292234659195, 1.222945613091875], Fake: [5.926588287353516, 1.315655711460268]) \n",
            "2000: D: 0.8519414067268372/0.4167225956916809 G: 1.3987765312194824 (Real: [3.994078150987625, 1.2494584508862059], Fake: [5.872465114593506, 1.3579930644997351]) \n",
            "2200: D: 1.037698745727539/0.3510071933269501 G: 0.9289397597312927 (Real: [4.005125144720077, 1.2826928145340883], Fake: [5.7068849420547485, 1.320456186186369]) \n",
            "2400: D: 0.5886189937591553/0.4041643738746643 G: 1.0659277439117432 (Real: [3.9610794496536257, 1.3032992472519178], Fake: [4.904763624668122, 1.371999810224236]) \n",
            "2600: D: 0.8606980443000793/0.7467944025993347 G: 0.8380826711654663 (Real: [4.030838016271591, 1.2155561542666034], Fake: [4.029253253936767, 1.230385820516895]) \n",
            "2800: D: 0.8266599774360657/0.9544150829315186 G: 0.5537681579589844 (Real: [3.9259459495544435, 1.1231387100329249], Fake: [3.274343717098236, 1.225980897254388]) \n",
            "3000: D: 0.6666648983955383/0.6704978346824646 G: 0.6334216594696045 (Real: [4.222889102622867, 1.33521180661327], Fake: [3.2761603033542634, 1.0915663820711774]) \n",
            "3200: D: 0.4318399727344513/0.9196245074272156 G: 0.5069684386253357 (Real: [3.95760049790144, 1.2445248507606979], Fake: [3.6032287633419036, 1.2368763168298254]) \n",
            "3400: D: 0.9632433652877808/0.5915048122406006 G: 0.6976393461227417 (Real: [4.0608476436138154, 1.1032826600130814], Fake: [4.313268699645996, 1.1475882437834277]) \n",
            "3600: D: 0.6059494614601135/0.9210996031761169 G: 0.8426679372787476 (Real: [3.981182963848114, 1.2589708242953235], Fake: [4.4892379403114315, 1.331282406288282]) \n",
            "3800: D: 0.6551298499107361/0.6744052171707153 G: 0.8031224608421326 (Real: [3.7932227057218553, 1.2384224450586205], Fake: [4.407818398475647, 1.548686205514434]) \n",
            "4000: D: 0.7667259573936462/0.747481107711792 G: 0.5881380438804626 (Real: [3.8317950920760633, 1.1928039776590251], Fake: [4.288844326734543, 1.380459437755774]) \n",
            "4200: D: 0.6172654032707214/0.7416370511054993 G: 0.8460482954978943 (Real: [3.9251222014427185, 1.3072990763501477], Fake: [3.9449757874011993, 1.3079582105567622]) \n",
            "4400: D: 0.5623567700386047/0.6395674347877502 G: 0.6150531768798828 (Real: [3.9296846586465835, 1.195110349969945], Fake: [3.619926965236664, 1.1418145632272474]) \n",
            "4600: D: 0.9229605197906494/0.4303014576435089 G: 1.156185507774353 (Real: [3.82837700933218, 1.2184371997359404], Fake: [3.9218970167636873, 1.0784756646378157]) \n",
            "4800: D: 0.6828469634056091/0.5531027913093567 G: 0.9426295757293701 (Real: [4.113405183553696, 1.1455040544440636], Fake: [3.726765866279602, 1.2141546009528545]) \n",
            "5000: D: 0.8396593928337097/0.5413820743560791 G: 0.5734745860099792 (Real: [3.773762129545212, 1.2024360769268079], Fake: [4.273710989952088, 1.3615306699438303]) \n",
            "5200: D: 0.935713529586792/0.7862108945846558 G: 0.7982792258262634 (Real: [4.0229528880119325, 1.0619203170630738], Fake: [4.016710424423218, 1.303996492241477]) \n",
            "5400: D: 0.8753358721733093/0.7034465670585632 G: 0.4534251391887665 (Real: [3.9778435522317888, 1.2072041636803956], Fake: [3.6252041232585905, 1.2886737556589256]) \n",
            "5600: D: 0.4652993679046631/0.6538252234458923 G: 0.8088571429252625 (Real: [4.157458501756191, 1.3503916698868381], Fake: [3.4691035521030424, 1.1880998927196877]) \n",
            "5800: D: 1.0046236515045166/0.707105815410614 G: 0.8636879324913025 (Real: [4.053356158137322, 1.4070448298933516], Fake: [3.8135625064373015, 1.380345773027024]) \n",
            "6000: D: 0.3422733545303345/1.3604388236999512 G: 0.4381215572357178 (Real: [3.865267784744501, 1.1860782139096346], Fake: [4.504753280878067, 1.2071439013563876]) \n",
            "6200: D: 0.7495794296264648/0.6348180174827576 G: 0.8331109285354614 (Real: [4.1114887601137164, 1.3385918993920323], Fake: [4.3970827078819275, 1.142040570353488]) \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}